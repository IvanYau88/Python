{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, criterion='entropy', max_depth=None, min_samples_leaf=1):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.tree = None\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"Calculate entropy of a target vector.\"\"\"\n",
        "        counts = np.bincount(y)\n",
        "        probabilities = counts / len(y)\n",
        "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
        "\n",
        "    def _gini(self, y):\n",
        "        \"\"\"Calculate Gini impurity of a target vector.\"\"\"\n",
        "        counts = np.bincount(y)\n",
        "        probabilities = counts / len(y)\n",
        "        return 1 - np.sum([p**2 for p in probabilities])\n",
        "\n",
        "    def _information_gain(self, y, y_left, y_right, criterion):\n",
        "        \"\"\"Calculate information gain for a split.\"\"\"\n",
        "        if criterion == 'entropy':\n",
        "            parent_impurity = self._entropy(y)\n",
        "            child_impurity = (len(y_left) / len(y)) * self._entropy(y_left) + (len(y_right) / len(y)) * self._entropy(y_right)\n",
        "        elif criterion == 'gini':\n",
        "            parent_impurity = self._gini(y)\n",
        "            child_impurity = (len(y_left) / len(y)) * self._gini(y_left) + (len(y_right) / len(y)) * self._gini(y_right)\n",
        "        return parent_impurity - child_impurity\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"Find the best feature and threshold to split on.\"\"\"\n",
        "        best_gain = -1\n",
        "        best_feature, best_threshold = None, None\n",
        "\n",
        "        for feature in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature] <= threshold\n",
        "                right_indices = X[:, feature] > threshold\n",
        "                if sum(left_indices) == 0 or sum(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                y_left, y_right = y[left_indices], y[right_indices]\n",
        "                gain = self._information_gain(y, y_left, y_right, self.criterion)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"Recursively build the decision tree.\"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        if (depth == self.max_depth or num_samples < self.min_samples_leaf or num_classes == 1):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feature, threshold = self._best_split(X, y)\n",
        "        if feature is None:\n",
        "            return Node(value=self._most_common_label(y))\n",
        "\n",
        "        left_indices = X[:, feature] <= threshold\n",
        "        right_indices = X[:, feature] > threshold\n",
        "\n",
        "        left = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature, threshold, left, right)\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        \"\"\"Return the most common label in a target vector.\"\"\"\n",
        "        return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the decision tree to the data.\"\"\"\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "        \"\"\"Predict a single sample by traversing the tree.\"\"\"\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._predict_sample(x, node.left)\n",
        "        else:\n",
        "            return self._predict_sample(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict labels for a dataset.\"\"\"\n",
        "        return np.array([self._predict_sample(x, self.tree) for x in X])\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "custom_tree_entropy = DecisionTree(criterion='entropy', max_depth=3, min_samples_leaf=2)\n",
        "custom_tree_entropy.fit(X_train, y_train)\n",
        "y_pred_custom_entropy = custom_tree_entropy.predict(X_test)\n",
        "print(f\"Custom Decision Tree (Entropy) Accuracy: {accuracy_score(y_test, y_pred_custom_entropy):.4f}\")\n",
        "\n",
        "custom_tree_gini = DecisionTree(criterion='gini', max_depth=3, min_samples_leaf=2)\n",
        "custom_tree_gini.fit(X_train, y_train)\n",
        "y_pred_custom_gini = custom_tree_gini.predict(X_test)\n",
        "print(f\"Custom Decision Tree (Gini) Accuracy: {accuracy_score(y_test, y_pred_custom_gini):.4f}\")\n",
        "\n",
        "sklearn_tree_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=2, random_state=42)\n",
        "sklearn_tree_entropy.fit(X_train, y_train)\n",
        "y_pred_sklearn_entropy = sklearn_tree_entropy.predict(X_test)\n",
        "print(f\"Sklearn Decision Tree (Entropy) Accuracy: {accuracy_score(y_test, y_pred_sklearn_entropy):.4f}\")\n",
        "\n",
        "sklearn_tree_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=2, random_state=42)\n",
        "sklearn_tree_gini.fit(X_train, y_train)\n",
        "y_pred_sklearn_gini = sklearn_tree_gini.predict(X_test)\n",
        "print(f\"Sklearn Decision Tree (Gini) Accuracy: {accuracy_score(y_test, y_pred_sklearn_gini):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuXDDOgNjdz7",
        "outputId": "93533c8d-445e-4ed9-8122-b96f263e91f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Decision Tree (Entropy) Accuracy: 0.9167\n",
            "Custom Decision Tree (Gini) Accuracy: 0.9444\n",
            "Sklearn Decision Tree (Entropy) Accuracy: 0.9167\n",
            "Sklearn Decision Tree (Gini) Accuracy: 0.9444\n"
          ]
        }
      ]
    }
  ]
}
